{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e63094f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "135c2aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    " spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"TwitterSentimentAnalysis\")\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ea05b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5cfe07ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 01:02:43 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "lines = spark.readStream\\\n",
    "        .format(\"socket\")\\\n",
    "        .option(\"host\", \"127.0.0.1\")\\\n",
    "        .option(\"port\", 3333)\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "278b3c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 01:02:45 WARN StateStore: Error managing HDFSStateStoreProvider[id = (op=0,part=165),dir = file:/private/var/folders/vw/8j0kbq6n5z7682q0r894_vxm0000gn/T/temporary-4fd2e3e1-97ce-4c0f-90cf-eb4fe9b3b57c/state/0/165], stopping management thread\n",
      "22/11/18 01:02:45 WARN StateStore: Error running maintenance thread\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef$.forExecutor(StateStoreCoordinator.scala:84)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.coordinatorRef(StateStore.scala:653)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.verifyIfStoreInstanceActive(StateStore.scala:635)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$doMaintenance$2(StateStore.scala:600)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$doMaintenance$2$adapted(StateStore.scala:598)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.doMaintenance(StateStore.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$startMaintenanceIfNeeded$1(StateStore.scala:582)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$MaintenanceTask$$anon$1.run(StateStore.scala:442)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://StateStoreCoordinator@172.30.1.18:56914\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n"
     ]
    }
   ],
   "source": [
    "tweet_df_string = lines.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "tweets_tab = tweet_df_string.withColumn('word', explode(split(col('value'), ' '))) \\\n",
    "    .groupBy('word') \\\n",
    "    .count() \\\n",
    "    .sort('count', ascending=False). \\\n",
    "    filter(col('word').contains('#'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eb682edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 01:02:49 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/vw/8j0kbq6n5z7682q0r894_vxm0000gn/T/temporary-6d912829-b008-4f30-b826-a518fc45977c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/18 01:02:49 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 01:02:58 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 8822 milliseconds\n",
      "22/11/18 01:03:20 WARN TextSocketMicroBatchStream: Stream closed by 127.0.0.1:3333\n"
     ]
    }
   ],
   "source": [
    "writeTweet = tweets_tab.writeStream. \\\n",
    "    outputMode(\"complete\"). \\\n",
    "    format(\"memory\"). \\\n",
    "    queryName(\"tweetquery\"). \\\n",
    "    trigger(processingTime='2 seconds'). \\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a32e5230",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
